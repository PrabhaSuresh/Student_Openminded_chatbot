{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jarvis\\anaconda3\\envs\\NLP\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fitz  # For PDF text extraction\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from accelerate import Accelerator  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ[\"TORCH_USE_CUDA_DSA\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator = Accelerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extracts text from a single PDF.\"\"\"\n",
    "    with fitz.open(pdf_path) as pdf:\n",
    "        text = \"\"\n",
    "        for page in pdf:\n",
    "            text += page.get_text()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, max_length=512, overlap=50):\n",
    "    \"\"\"Chunks a large text into smaller segments.\"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    chunk = []\n",
    "    for word in words:\n",
    "        chunk.append(word)\n",
    "        if len(chunk) >= max_length:\n",
    "            chunks.append(\" \".join(chunk))\n",
    "            chunk = chunk[-overlap:]  # Retain the overlap for the next chunk\n",
    "    if chunk:\n",
    "        chunks.append(\" \".join(chunk))\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_multiple_pdfs(pdf_paths):\n",
    "    all_chunks = {}\n",
    "    for pdf_path in pdf_paths:\n",
    "        print(f\"Processing: {pdf_path}\")\n",
    "        pdf_text = extract_text_from_pdf(pdf_path)\n",
    "        chunks = chunk_text(pdf_text)\n",
    "        all_chunks[pdf_path] = chunks  # Store chunks for each PDF\n",
    "    return all_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_paths = [ \n",
    "    r\"D:\\Prabha\\NLP\\Dataset\\le8\\le_8.pdf\",\n",
    "    r\"D:\\Prabha\\NLP\\Dataset\\le9\\le9.pdf\",\n",
    "    r\"D:\\Prabha\\NLP\\Dataset\\le10\\le10 (2).pdf\",\n",
    "    r\"D:\\Prabha\\NLP\\Dataset\\le11\\le11.pdf\",\n",
    "    r\"D:\\Prabha\\NLP\\Dataset\\le12\\le12.pdf\",\n",
    "    r\"D:\\Prabha\\NLP\\Dataset\\le13\\le13.pdf\",\n",
    "    r\"D:\\Prabha\\NLP\\Dataset\\le14\\le14.pdf\",\n",
    "    r\"D:\\Prabha\\NLP\\Dataset\\le15\\le15.pdf\",\n",
    "    r\"D:\\Prabha\\NLP\\Dataset\\le1\\Tamilnadu-Board-Class-12-Chemistry-Chapter-1.pdf\",\n",
    "    r\"D:\\Prabha\\NLP\\Dataset\\le2\\Tamilnadu-Board-Class-12-Chemistry-Chapter-2.pdf\",\n",
    "    r\"D:\\Prabha\\NLP\\Dataset\\le3\\Tamilnadu-Board-Class-12-Chemistry-Chapter-3.pdf\",\n",
    "    r\"D:\\Prabha\\NLP\\Dataset\\le4\\Tamilnadu-Board-Class-12-Chemistry-Chapter-4.pdf\",\n",
    "    r\"D:\\Prabha\\NLP\\Dataset\\le5\\12th_Chemistry-Vol-2_English Medium_Text.pdf\",\n",
    "    r\"D:\\Prabha\\NLP\\Dataset\\le6\\Tamilnadu-Board-Class-12-Chemistry-Chapter-6.pdf\",\n",
    "    r\"D:\\Prabha\\NLP\\Dataset\\le7\\Tamilnadu-Board-Class-12-Chemistry-Chapter-7.pdf\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: D:\\Prabha\\NLP\\Dataset\\le8\\le_8.pdf\n",
      "Processing: D:\\Prabha\\NLP\\Dataset\\le9\\le9.pdf\n",
      "Processing: D:\\Prabha\\NLP\\Dataset\\le10\\le10 (2).pdf\n",
      "Processing: D:\\Prabha\\NLP\\Dataset\\le11\\le11.pdf\n",
      "Processing: D:\\Prabha\\NLP\\Dataset\\le12\\le12.pdf\n",
      "Processing: D:\\Prabha\\NLP\\Dataset\\le13\\le13.pdf\n",
      "Processing: D:\\Prabha\\NLP\\Dataset\\le14\\le14.pdf\n",
      "Processing: D:\\Prabha\\NLP\\Dataset\\le15\\le15.pdf\n",
      "Processing: D:\\Prabha\\NLP\\Dataset\\le1\\Tamilnadu-Board-Class-12-Chemistry-Chapter-1.pdf\n",
      "Processing: D:\\Prabha\\NLP\\Dataset\\le2\\Tamilnadu-Board-Class-12-Chemistry-Chapter-2.pdf\n",
      "Processing: D:\\Prabha\\NLP\\Dataset\\le3\\Tamilnadu-Board-Class-12-Chemistry-Chapter-3.pdf\n",
      "Processing: D:\\Prabha\\NLP\\Dataset\\le4\\Tamilnadu-Board-Class-12-Chemistry-Chapter-4.pdf\n",
      "Processing: D:\\Prabha\\NLP\\Dataset\\le5\\12th_Chemistry-Vol-2_English Medium_Text.pdf\n",
      "Processing: D:\\Prabha\\NLP\\Dataset\\le6\\Tamilnadu-Board-Class-12-Chemistry-Chapter-6.pdf\n",
      "Processing: D:\\Prabha\\NLP\\Dataset\\le7\\Tamilnadu-Board-Class-12-Chemistry-Chapter-7.pdf\n"
     ]
    }
   ],
   "source": [
    "chunks = process_multiple_pdfs(pdf_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jarvis\\AppData\\Local\\Temp\\ipykernel_2224\\1394356449.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  hf_embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n"
     ]
    }
   ],
   "source": [
    "hf_embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vector_store = FAISS.from_texts(texts=chunks, embedding=hf_embeddings)\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = r\"D:\\Prabha\\NLP\\Model\\gen_ai_chatbot\\model\\output\\model_6\\save\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(r\"D:\\Prabha\\NLP\\Model\\gen_ai_chatbot\\model\\output\\model_6\\save\\tokenizer\", pad_token=\"[PAD]\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "llm_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=512,  \n",
    "    max_new_tokens=256,\n",
    "    truncation=True,\n",
    "    return_full_text=False,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jarvis\\AppData\\Local\\Temp\\ipykernel_2224\\1373755132.py:2: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFacePipeline`.\n",
      "  llm = HuggingFacePipeline(pipeline=llm_pipeline)\n",
      "C:\\Users\\Jarvis\\AppData\\Local\\Temp\\ipykernel_2224\\1373755132.py:3: LangChainDeprecationWarning: This class is deprecated. See the following migration guides for replacements based on `chain_type`:\n",
      "stuff: https://python.langchain.com/v0.2/docs/versions/migrating_chains/stuff_docs_chain\n",
      "map_reduce: https://python.langchain.com/v0.2/docs/versions/migrating_chains/map_reduce_chain\n",
      "refine: https://python.langchain.com/v0.2/docs/versions/migrating_chains/refine_chain\n",
      "map_rerank: https://python.langchain.com/v0.2/docs/versions/migrating_chains/map_rerank_docs_chain\n",
      "\n",
      "See also guides on retrieval and question-answering here: https://python.langchain.com/v0.2/docs/how_to/#qa-with-rag\n",
      "  combine_documents_chain = load_qa_chain(llm=llm, chain_type=\"stuff\")\n",
      "C:\\Users\\Jarvis\\AppData\\Local\\Temp\\ipykernel_2224\\1373755132.py:4: LangChainDeprecationWarning: This class is deprecated. Use the `create_retrieval_chain` constructor instead. See migration guide here: https://python.langchain.com/v0.2/docs/versions/migrating_chains/retrieval_qa/\n",
      "  rag_chain = RetrievalQA(combine_documents_chain=combine_documents_chain, retriever=retriever)\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = accelerator.prepare(model, tokenizer)\n",
    "llm = HuggingFacePipeline(pipeline=llm_pipeline)\n",
    "combine_documents_chain = load_qa_chain(llm=llm, chain_type=\"stuff\")\n",
    "rag_chain = RetrievalQA(combine_documents_chain=combine_documents_chain, retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = input(\"Enter your query: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: what is glucose?\n",
      "\n",
      "Answer:\n",
      "Glucose is the universal monosaccharide. It is present in blood and various\n",
      "degrees and types of cells. It is formed from the following five\n",
      "commonly-Used carbon-carbon bonds i.e. Carbon-carbon-bond, carbon-carbon-double bond, carbon-carbon-single bond, carbon-carbon-bond, carbon-carbon- double bond.\n",
      "Source: Wikipedia. http://www.wikipedia.gov/\n",
      "\n",
      "A:\n",
      "\n",
      "Glucose is the carbon-carbon-bond\n",
      "    Carbon-carbon-double bond\n",
      "Carbon-carbon-single bond\n",
      "    Carbon-carbon-bond\n",
      "Carbon-carbon-bond\n",
      "Carbon-carbon-double bond\n",
      "\n",
      "A:\n",
      "\n",
      "Glucose is the carbon-carbon-bond\n",
      "Carbon-carbon-single bond\n",
      "Carbon-carbon-bond\n",
      "Carbon-carbon-double bond\n",
      "\n",
      "A:\n",
      "\n",
      "Glucose is the monosaccharide\n",
      "    carbon-carbon-bond\n",
      "Carbon-carbon-double bond\n",
      "\n",
      "\n",
      "\n",
      "Feel free to ask more questions or clarify details.\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"query\": query})\n",
    "\n",
    "def clean_response(response):\n",
    "    output = f\"Question: {response['query']}\\n\\n\"\n",
    "    output += f\"Answer:{response['result']}\\n\\n\"\n",
    "    output += \"Feel free to ask more questions or clarify details.\"\n",
    "    return output\n",
    "\n",
    "print(clean_response(response))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
